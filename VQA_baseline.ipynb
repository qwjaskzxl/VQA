{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思路\n",
    "1. 数据处理，构造数据集\n",
    "    - 文本：K个常见ans\n",
    "    - 图像：\n",
    "- 模型\n",
    "    - image：resize->cnn->fc_1024\n",
    "    - question：embedding->fc_1024\n",
    "    - concat or add->fc_K->交叉熵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "SEED = 1234\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-328b56bffdf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./dataset/mscoco_val2014_annotations.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./dataset/MultipleChoice_mscoco_val2014_questions.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/mscoco_val2014_annotations.json'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/mscoco_val2014_annotations.json'",
     "output_type": "error"
    }
   ],
   "source": [
    "import json\n",
    "with open('./dataset/mscoco_val2014_annotations.json') as f1, open('./dataset/MultipleChoice_mscoco_val2014_questions.json') as f2:\n",
    "    annotations = json.load(f1)\n",
    "    questions = json.load(f2) \n",
    "\n",
    "# with open('./dataset/v2_mscoco_val2014_annotations.json') as f1, open('./dataset/v2_OpenEnded_mscoco_val2014_questions.json') as f2:\n",
    "#     annotations = json.load(f1)\n",
    "#     questions = json.load(f2) \n",
    "# with open('./dataset/OpenEnded_mscoco_val2014_questions.json') as f:\n",
    "#     question = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[解析JSON](https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p02_read-write_json_data.html)，先观察数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方API中vqaTools demo："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from vqaTool import VQA\n",
    "\n",
    "dataDir\t\t='./dataset'\n",
    "versionType ='' # this should be '' when using VQA v2.0 dataset\n",
    "taskType    ='MultipleChoice' # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
    "dataType    ='mscoco'  # 'mscoco' only for v1.0. 'mscoco' for real and 'abstract_v002' for abstract for v1.0.\n",
    "dataSubType ='val2014'\n",
    "annFile     ='%s/%s%s_%s_annotations.json'%(dataDir, versionType, dataType, dataSubType)\n",
    "quesFile    ='%s/%s%s_%s_%s_questions.json'%(dataDir, versionType, taskType, dataType, dataSubType)\n",
    "# imgDir \t\t= '%s/Images/%s/%s/' %(dataDir, dataType, dataSubType)\n",
    "img_dir = '%s/Images/%s/' %(dataDir, dataSubType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ann = annotations['annotations'][:]\n",
    "ques = questions['questions'][:]\n",
    "print('num of ques:', len(ques))\n",
    "\n",
    "QUES_N = len(ques) \n",
    "ANS_K = 999 #最常见的ans数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "display(ann[5])\n",
    "display(ques[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造数据集\n",
    "做完基本处理后放dataloader里，可以并行  \n",
    "**（1）处理文本**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#提取ques ans imgId\n",
    "from collections import Counter\n",
    "ground_truth_trainset = []\n",
    "ques_trainset = []\n",
    "img_ids = []\n",
    "\n",
    "for i in range(QUES_N):\n",
    "    ground_truth_trainset.append(ann[i]['multiple_choice_answer'])\n",
    "    ques_trainset.append(ques[i]['question'])\n",
    "    if i % 3 == 0:\n",
    "        img_ids.append(ques[i]['image_id'])\n",
    "        \n",
    "print('num of images：', len(img_ids))\n",
    "ans_words = dict(Counter(ground_truth_trainset))\n",
    "print('num of ans:', len(ans_words))\n",
    "ans_words_most = dict(Counter(ground_truth_trainset).most_common(ANS_K))#最常见的1000个ans作分类\n",
    "\n",
    "print(ground_truth_trainset[:3], ques_trainset[:3], img_ids[:1], sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('en')\n",
    "# return nlp.tokenizer(text)\n",
    "\n",
    "def word_tokenize(text):\n",
    "    t = [w for w in text.split(' ')]\n",
    "    t[-1] = t[-1][:-1]\n",
    "    return t\n",
    "\n",
    "word2idx = {'UNK':0}\n",
    "ans2idx = {}\n",
    "v, v_a = 1, 1\n",
    "\n",
    "for i in range(QUES_N):\n",
    "    for w in word_tokenize(ques_trainset[i]):\n",
    "        if w not in word2idx:\n",
    "            word2idx[w] = v\n",
    "            v += 1\n",
    "    ques_trainset[i] = torch.LongTensor([word2idx[w] for w in word_tokenize(ques_trainset[i])])\n",
    "\n",
    "for i,w in enumerate(ground_truth_trainset):\n",
    "    if w not in word2idx:\n",
    "        word2idx[w] = v\n",
    "        v += 1\n",
    "    if w in ans_words_most:\n",
    "        if w not in ans2idx:\n",
    "            ans2idx[w] = v_a\n",
    "            v_a += 1\n",
    "        ground_truth_trainset[i] = torch.LongTensor([ans2idx[w]])\n",
    "    \n",
    "    else:\n",
    "        ground_truth_trainset[i] = torch.LongTensor([0])\n",
    "        \n",
    "print('vocab_size:',v)\n",
    "print('true num of ans:',v_a)\n",
    "VOCAB_SIZE = v\n",
    "idx2word = {i:w for i,w in enumerate(word2idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "s,m = 0,0\n",
    "for i,x in enumerate(ques_trainset):\n",
    "    s+=len(x)\n",
    "    if len(x)>m:\n",
    "        m=len(x)\n",
    "print('句长mean:%f, max:%d'%(s/i, m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** （2）处理图像 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt#为什么写到一个框里就不能显示\n",
    "from PIL import Image\n",
    "import skimage.io as io\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "def trans_img(path, size=IMG_SIZE):\n",
    "    mode = Image.open(path)\n",
    "    if len(mode.split()) == 3:\n",
    "        transform1 = transforms.Compose([\n",
    "            transforms.Resize((size, size)),#确保无黑边\n",
    "            transforms.CenterCrop((size, size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    else:\n",
    "        transform1 = transforms.Compose([\n",
    "            transforms.Resize((size, size)),#确保无黑边\n",
    "            transforms.CenterCrop((size, size)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    mode = transform1(mode)\n",
    "    return mode\n",
    "\n",
    "def show_img(image):\n",
    "    mode = transforms.ToPILImage()(image)\n",
    "    plt.imshow(mode)\n",
    "    plt.show()\n",
    "    \n",
    "for img_id in img_ids[:1]:\n",
    "    img_filename = 'COCO_' + dataSubType + '_'+ str(img_id).zfill(12) + '.jpg'\n",
    "    img = trans_img(img_dir + img_filename)\n",
    "    print(img_id, img.shape)\n",
    "    show_img(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（3）DataLoader**  \n",
    "后来发现可以写一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 1024 # 2:128, 3:92, 4:83, 5/1000:73/76, 50/8:64/66, \n",
    "MAX_SEQ_LEN = 23\n",
    "\n",
    "class VQA_Dataset(Dataset):\n",
    "    def __init__(self, img_ids, q, a):\n",
    "        self.img_ids = img_ids\n",
    "        self.q = q            \n",
    "        self.a = a\n",
    "        \n",
    "    def __len__(self):\n",
    "        return QUES_N\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = 'COCO_' + dataSubType + '_'+ str(img_ids[idx//3]).zfill(12) + '.jpg'\n",
    "        img = trans_img(img_dir + img_filename)\n",
    "        Lq = len(self.q[idx])\n",
    "        if Lq > MAX_SEQ_LEN:\n",
    "            self.q[idx] = self.q[idx][:MAX_SEQ_LEN]\n",
    "        elif Lq < MAX_SEQ_LEN:\n",
    "            self.q[idx] = F.pad(self.q[idx], (0,MAX_SEQ_LEN-Lq))#前后分别补多少\n",
    "        return img, self.q[idx], self.a[idx]\n",
    "    \n",
    "dataset = VQA_Dataset(img_ids=img_ids, q=ques_trainset, a=ground_truth_trainset)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=False, num_workers=40)\n",
    "# return list[img,q,a], img:[b,3,256,256], q:[b,seq], a:[b,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "EPOCH = 1\n",
    "\n",
    "class VQA_baseline(nn.Module):\n",
    "    def __init__(self, embed_dim, fc_dim, out_dim):\n",
    "        super(VQA_baseline, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 10, 5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.img_fc = nn.Linear(in_features=10*61*61, out_features=fc_dim)\n",
    "        \n",
    "        self.embed = nn.Embedding(VOCAB_SIZE, embed_dim)\n",
    "        self.ques_fc = nn.Linear(MAX_SEQ_LEN*embed_dim, fc_dim)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self.fc2ans = nn.Linear(fc_dim, out_dim)\n",
    "        \n",
    "    def forward(self, p, q):\n",
    "        # pic:[b,3,256,256]\n",
    "        p = self.pool(F.relu(self.conv1(p))) #[b,6,252,252]->[b,6,126,126]\n",
    "        p = self.pool(F.relu(self.conv2(p))) #[b,10,122,122]->[b,10,61,61]\n",
    "        p = p.view(-1, 10*61*61) #[b,10*61*61]\n",
    "        p = self.bn(self.img_fc(p)) #[b,fc_dim]\n",
    "        \n",
    "        # ques:[b,seq]\n",
    "        q = q.view(MAX_SEQ_LEN, -1)#[seq,b]，因为并行时，batch_size会被打开\n",
    "        q = self.embed(q)#[seq,emb,b]\n",
    "        q = q.view(-1, MAX_SEQ_LEN * self.embed_dim)\n",
    "        \n",
    "        q = self.ques_fc(q)\n",
    "        q = self.bn(q)#[b,fc_dim]\n",
    "        \n",
    "        return self.fc2ans(p+q) #element-wise add, [b,out_dim]\n",
    "    \n",
    "    \n",
    "model = VQA_baseline(embed_dim=300, fc_dim=1024, out_dim=1000)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    model = nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "t = time()\n",
    "for epoch in range(EPOCH):\n",
    "    for i, (img, q, a) in enumerate(dataloader):\n",
    "        opt.zero_grad()\n",
    "        if USE_CUDA:\n",
    "            img,q,a = img.cuda(),q.cuda(),a.cuda()\n",
    "        \n",
    "        y = model(img, q)\n",
    "        loss = loss_fn(y, a.squeeze())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print('epoch:',epoch+1,'iter:',i,'loss:',loss.item())\n",
    "    print(time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_path = './models/VQA.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img[:2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "it  = iter(dataloader)\n",
    "d = next(it) #就是一个batch\n",
    "d.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (img, q, a) in dataloader:\n",
    "        outputs = model(img, q)\n",
    "        _, predicted = torch.max(outputs.data, 1)#返回value, index\n",
    "        total += a.size(0)\n",
    "        correct += (predicted.cpu() == a.squeeze()).sum().item()\n",
    "\n",
    "print(correct/total)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img[:2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "it  = iter(dataloader)\n",
    "d = next(it) #就是一个batch\n",
    "d.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (img, q, a) in dataloader:\n",
    "        outputs = model(img, q)\n",
    "        _, predicted = torch.max(outputs.data, 1)#返回value, index\n",
    "        total += a.size(0)\n",
    "        correct += (predicted.cpu() == a.squeeze()).sum().item()\n",
    "\n",
    "print(correct/total)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "img[:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "it  = iter(dataloader)\n",
    "d = next(it) #就是一个batch\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "d[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (img, q, a) in dataloader:\n",
    "        outputs = model(img, q)\n",
    "        _, predicted = torch.max(outputs.data, 1)#返回value, index\n",
    "        total += a.size(0)\n",
    "        correct += (predicted.cpu() == a.squeeze()).sum().item()\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}